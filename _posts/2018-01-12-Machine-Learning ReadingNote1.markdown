---
layout:     post
title:      "读书笔记 Machine Learning #1"
subtitle:   "模型的评估与选择"
date:       2018-01-12
author:     "bluebird"
header-img: "img/post-bg-unix-linux.jpg"
tags:
    - Machine Learning
    - Reading Note
---

> “新手上路. ”

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

## 1. 经验误差和过拟合

|  定义  |          英文          |             描述              |
| :--: | :------------------: | :-------------------------: |
| 错误率  |      error rate      |  在m个样本中有a个样本分类错误，则错误率E=a/m  |
|  精度  |       accuracy       |  在m个样本中有a个样本分类错误，则精度为1-a/m  |
|  误差  |        error         |    学习器实际预测的值和样本实际值之间的差距     |
| 训练误差 |    training error    |         在训练集上出现的误差          |
| 泛化误差 | generalization error |    在新样本的误差（我们希望泛华误差越小越好）    |
| 欠拟合  |     underfitting     | 学习器的学习能力不足，没有将训练集的一般特性学习充分  |
| 过拟合  |     overfitting      | 学习器学习能力过强，把训练集自身的特性当做一般特性学习 |



## 2. 评估方法

|  定义  |       英文       |              描述               |
| :--: | :------------: | :---------------------------: |
| 训练集  |  training set  |         用来训练学习器的数据集合          |
| 测试集  |  testing set   |     用来测试学习器对新样本判别能力的数据集合      |
| 验证集  | validation set |    在模型评估和与选择中，用于评估测试的数据集合     |
| 测试误差 | testing error  | 我们通常用学习器在测试集上的测试误差，来作为泛化误差的近似 |

假设我们有包含m个样例的数据集$$D=\{(x_1,y_1),(x_2,y_2),…,(x_m,y_m)\}$$如果我们既要用这个数据集下进行训练，又要用这个数据集进行测试，我们就需要对D进行一定的处理，从中产生出训练集S和测试集T。

1. 留出法(hold-out)

   将D划分成两个互斥的集合，一个作为训练集S，另外一个用作测试集T，在S上训练出模型后，用T来评估它的测试误差，$$D=S\cup T,S\cap T=\emptyset$$

   注意点1：训练集和测试集要尽可能保持数据分布的一致性，比如D包含1000个样本，500个正例，500个反例，要求70%样本作为训练集，30%样本作为测试集，那么700个训练用样本中，应该350个正例，350个反例，300个测试用样本中，应该150个正例，150个反例。

   注意点2：即使按比例划分，不同的样本组合（比如哪350个放进训练集）也会带来不同的结果，所以单次留出法的结果往往不够可靠，往往使用**若干次随机划分**，重复进行实验评估后取平均值作为留出法的评估结果。

2. 交叉验证法(cross validation)

   使用分层采样，将数据集D划分为k个大小相似的互斥子集，即 $$D=D_1\cup D_2 \cup … \cup D_k ,D_j \cap D_i = \emptyset (i \neq j)$$每次用k-1个子集的并集作为训练集，用用剩余的一个子集作为测试集，总共可进行k此训练和测试，因而把交叉验证法，成为k折交叉验证法(k-fold cross validation)

   注意点1：与留出法相似，将D划分成k个子集有多种划分方式，为减少样本因素的影响，k折交叉要使用随机划分重复p次，最终的结果是p次k折交叉结果的均值

   注意点2：留一法(Leave-Out-One)，k折交叉验证的一个特例，k=m，即每个子集只包含一个样本。评估结果往往被认为比较准确但是如果样本量大，需要很大的计算开销，需要程序员进行折中的考量

3. 自助法(bootstrapping)

   使用自助采样法，在包含m个样本的数据集D中，进行m次有放回的随机采样，得到新的数据集D‘，这样的做的结果是D中有一部分数据会在D’中出现多次，有些数据不会出现，不会出现的比例约为0.368，也就是约有1/3的数据没有在训练集中出现，而这些数据用作测试集，这样的测试结果被称为**包外估计**(out-of-bag estimate)

   注意点1：自助法适合数据集小，难以有效划分训练集/测试集的情况，但是它改变了 初始数据的分布，引入估计偏差，因而当数据充足时还是使用留出法和交叉验证法。

   证明：样本在m次采样中，始终不被采到的概率是$$(1-\frac{1}{m})^m$$,取极限可以得到$$lim_{m\to+\infty} (1-\frac{1}{m})^m =  \frac{1}{e} \approx 0.386$$

4. 调参和最终模型

   大多数的学习算法都有参数(paramerter)需要设定，参数的配置不同，学得模型的性能往往有很大的差别，所以在模型的评估和选择时，也要对算法参数进行设定，这就是通常说的参数调节，也就是调参(parameter tuning)



## 性能度量

性能度量（performance measure）是对模型泛化能力的评价标准

1. f表示学习器
2. m个样例的数据集$$D=\{(x_1,y_1),(x_2,y_2),…,(x_m,y_m)\}$$
3. f(x)作为预测结果
4. 假设数据服从分布$$\delta$$，概率密度为$$p(\bullet)$$

#### 回归任务

回归任务最常用的性能度量是**均方误差**(mean squared error) 

|  描述   |                    公式                    |
| :---: | :--------------------------------------: |
| 带分布律  | $$E(f;\delta)=\int_{x\sim\delta} (f(x)-y)^2p(x)\,dx.$$ |
| 不带分布律 | $$E(f;D)=\frac{1}{m}\sum_{i=1}^m (f(x_i)-y_i)^2$$ |

#### 分类任务

1. 错误率与精度

   |   描述    |                    公式                    |
   | :-----: | :--------------------------------------: |
   |   错误率   | $$E(f;D)=\frac{1}{m}\sum_{i=1}^m \mathbb I(f(x_i) \neq y_i)$$ |
   |   精度    | $$E(f;D)=\frac{1}{m}\sum_{i=1}^m \mathbb I(f(x_i = y_i))$$ |
   | 带分布率错误率 | $$E(f;\delta)=\int_{x\sim\delta} \mathbb I(f(x) \neq y)p(x)\,dx.$$ |
   | 带分布律精度  | $$E(f;\delta)=\int_{x\sim\delta} \mathbb I(f(x) = y)p(x)\,dx.$$ |

2. 查准率，查全率和F1

   |      定义      |       描述       |        例子         |
   | :----------: | :------------: | :---------------: |
   | 查准率(percise) | 查询出来的结果有多少的正确的 | 检索出的信息有多少是用户干感兴趣的 |
   | 查全率(recall)  | 在多有正确的数据中，有多少  | 用户感兴趣的数据有多少被查出来了  |

   定义二分类问题中de 混淆矩阵

   | 真实结果 |  预测正例   |  预测反例   |
   | :--: | :-----: | :-----: |
   |  正例  | TP（真正例） | FN（假反例） |
   |  反例  | FP（假正例） | TN（真反例） |

   我们可以得到公式

   |  定义  |           公式           |
   | :--: | :--------------------: |
   | 查准率P | $$P=\frac{TP}{TP+FP}$$ |
   | 查全率R | $$R=\frac{TP}{TP+FN}$$ |

   一般来说P高，R往往偏低，R高，P往往偏低，从极限的角度来考虑，如果将所有西瓜都选中，那么好瓜必然都选上，但是查准率就会降低查准率，如果选特别确定的西瓜，那么就会漏掉很多好瓜。

   我们根据预测结果对样本进行排序，前面是**最可能**的正例样本，按顺序朱哥把样本算作正例预测，可以得出多组，查准率和查全率，以查准率为纵轴，查全全率为横轴，可以得到P-R曲线，（图像来自周志华老师的教材）

   ![PR](https://github.com/wshwbluebird/wshwbluebird.github.io/raw/master/ML_img/PR.jpg)

    **平衡点**(Break_Even Point)简称BEP，是P=R的一个取值，用来衡量不同学习器在这两个指标上的优劣

   BEP太过简单，因而引入F1度量

   $$F1=\frac{2 \times P \times R}{P + R} = \frac{2 \times TP}{样例总数 + TP - TN}$$

   在不同系统中，我们队查准率和查重率有不同的偏好，所以引入F1度量的一般形式$$F_\beta$$

   $$F_\beta = \frac {(1+\beta^2)\times P \times R}{(\beta^2 \times P) + R} , \beta \gt 0$$  其中 $$\beta \gt 1$$  查全率影响大，其中 $$\beta \lt 1$$  查准率影响大

   如果在数据上进行多次的训练测试，或在多个数据集上进行训练和测试，那么可能会得到很多组查全率和查准率，这是我们可以计算均值得到对应的**宏查全率**，**宏查准率**，**宏查F1**（macro）

3. ROC与AUC

   截断点(cut point) ：根据样本产生出来的预测值进行排序，把最可能排在前面，最不可能排在后面，这个时候选择一个截断点，断点之前的样本被预测为正例，断点之后的样本被预测为反例。

    ROC全程**受试者工作特征**(Receiver Operating Characteristic)，与上面介绍的P-R曲线类似根据学习器的预测结果，对样例进行排序，然后按顺序逐个把样本作为正例进行预测，每次计算两个重要的值，ROC的纵轴是真正例率，横轴的是假正例率。

   |    定义     |            公式            |
   | :-------: | :----------------------: |
   | 真正例率(TPR) | $$TPR=\frac{TP}{TP+FN}$$ |
   | 假正例率(FPR) | $$FPR=\frac{FP}{TN+FQ}$$ |

   ROC示例图，图来自网络

   ![ROC](https://github.com/wshwbluebird/wshwbluebird.github.io/raw/master/ML_img/ROC.jpg)

   为评价学习的优劣，我们以AUC(Area Under Roc Curve)作为判断依据，AUC就是ROC曲线下的面积。

   ROC的两种估算方式：

   1. 假定ROC曲线是有坐标为$$\{(x_1,y_1),(x_2,y_2),…,(x_m,y_m)\}$$的点连成的曲线，则AUC可以估算为$$AUC=\frac{1}{2}\sum_{i=1}^{m-1}(x_{i+1} - x_i)\cdot(y_i + y_{i+1}) $$

   2. 形式化的看，AUC考虑的是样本预测的排序质量，因此与排序误差紧密相关给定$$m^+$$个正例$$m^-$$个反例，其中$$D^+$$和$$D^-$$分别代表正，反例的集合，则排序的**损失**(loss)定义为

      $$\mathfrak l_{rank} = \frac{1}{m^+m^-}\sum_{x^+\in D^+}\sum_{x^-\in D^-}(\mathbb I(f(x^+)\lt f(x^-))+\frac{1}{2}\mathbb I(f(x^+)=f(x^-)))$$

      对于每一对正例和反例，若正例的预测值小于反例值（预测不正确），则记一个**罚分**，若正例的预测值等于反例值（不准确），则记半个罚分，因而$$\mathfrak l_{rank} $$对应的是ROC曲线之上的面积，因此有$$AUC = 1 - \mathfrak l_{rank} $$

4. 代价敏感错误与代价曲线

   并非错误的代价都是一样的，把反例预测成正例与把正例预测成反例都是犯了一次错误，但是他们的代价可能是不一样的。为权衡不同类型的错误造成的不同损失，可以为错误赋予**非均等代价**(unequal cost)，因此我们可以定义一个代价矩阵

   | 真实类别 |    预测为第0类     |    预测为第1类     |
   | :--: | :-----------: | :-----------: |
   | 第0类  |       0       | $$cost_{01}$$ |
   | 第1类  | $$cost_{10}$$ |       0       |

   $$cost_{ij}$$表示把第i种类型预测为第j种类型所出现的错误，根据上表重新定义的**代价敏感**(cost-sensitive)错误率为

   $$E(f;D;cost)=\frac{1}{m}(\sum_{x_i\in D^+}\mathbb I(f(x_i)\neq y_i)\times cost_{01}+\sum_{x_i\in D^-}\mathbb I(f(x_i)\neq y_i)\times cost_{10})$$

   在代价敏感的条件下ROC曲线不能反映学习器期望的总代价，这里使用**代价曲线**(cost curve)，其中p是样例为正例的概率

   |  坐标  |   描述    |  值域   |                    公式                    |
   | :--: | :-----: | :---: | :--------------------------------------: |
   |  横轴  | 正例概率的代价 | [0,1] | $$P(+)cost=\frac{p\times cost_{01}}{p\times cost_{01}+(p\times cost_{10})}$$ |
   |  纵轴  | 归一化的代价  | [0,1] | $$cost_{norm}=\frac{FNR\times p\times cost_{01}+FPR\times (1-p)\times cost_{10}}{p\times cost_{01}+(p\times cost_{10})}$$ |

   之后我们将ROC曲线上的点，转化成代价曲线图中的线段，假设原ROC上的点为(FPR,TPR)，首先根据$$FNR=1-TPR ,$$计算出FNR，然后根据两点(0,FPR)(1,FNR)画出线段，线段下的面积就是该条件下期望的总代价，类似将所有的点都转化为线段，围出来的面积就是期望的总体代价



## 比较检验

机器学习算法性能比较非常复杂，我们希望比较是泛华的性能，而通过实验评估得到的是测试集的性能，因而不同测试集出现的结果对比可能不尽相同，而在某个测试集上的性能与测试集本身的特性有关。我们通常使用**假设检验**(hypothesis test)来作为比较学习器性能的重要的依据

这个就是普通的假设检验，有很多种方式，具体可以参考《概率论》相关教材

#### 1. 假设检验

1. 做一次留出法实验，假设分类器错误分类的概率$$\varepsilon \le 0.3$$ ，二项分布
2. 做k次留出实验，假设分类器错误分类的概率$$\varepsilon = \varepsilon_0$$ , 平均错误率$$\mu$$和方差$$\sigma^2$$，则$$\frac{\sqrt{k}(\mu-\varepsilon_0)}{\sigma}$$符合自由度为k-1的t分布

#### 2. 交叉验证t检验

​	比较两个学习器A和B，两个算法做k折交叉验证法，得到错误率分别为$$\varepsilon_1^A,\varepsilon_2^A,..,\varepsilon_k^A$$和$$\varepsilon_1^B,\varepsilon_2^B,..,\varepsilon_k^B$$使用成对t检验来比较两个算法的好坏。由$$\Delta_i =\varepsilon_i^A - \varepsilon_i^B  $$，得到$$\Delta_1,\Delta_2,…,\Delta_k$$，计算这些差值得均值$$\mu$$和方差$$\sigma^2$$，在"学习器A和学习B性能相同的情况下"，$$|\frac{\sqrt{k}\mu}{\sigma}|$$符合自由度为k-1的t分布

#### 3. McNemar检验

​	使用留出法处理二分类问题，对于两个学习器A和B，得到它们的测试错误率和学习结果差别，两者都正确，一个正确一个错误，全部错误的样本数，得到级联列表

| 算法B  |   算法A正确    |   算法A错误    |
| :--: | :--------: | :--------: |
|  正确  | $$e_{00}$$ | $$e_{01}$$ |
|  错误  | $$e_{10}$$ | $$e_{11}$$ |

如果两个学习器学习能力相同，应该有$$e_{10}=e_{10}$$，则$$\frac{(|e_{10}-e_{10}|-1)^2}{e_{10}+e_{10}}$$ 符合自由度为1的卡方分布

#### 4. Friedman检验与Nemenyi后续检验

​	比较多个算法的水平 ，假设我们使用$$D_1,D_2,D_3,D_4$$四个数据集对算法A,B,C进行比较，首先使用留出法或其他方法得到在每个数据集上，算法好坏的排名，如果排名相同取平均值，可以得到算法比较序值表，假设如下表所示

|   数据集   | 算法A  |  算法B  |  算法C  |
| :-----: | :--: | :---: | :---: |
| $$D_1$$ |  1   |   2   |   3   |
| $$D_2$$ |  1   |  2.5  |  2.5  |
| $$D_3$$ |  1   |   2   |   3   |
| $$D_4$$ |  1   |   2   |   3   |
|  平均序值   |  1   | 2.125 | 2.875 |

令$$r_i$$表示di个算法的平均序值，则可以根据以下分布进行检测，假设是所有算法性能没有显著区别

1. 保守法：$$\Gamma_{\mathfrak x^2}\frac{12N}{k(k+1)}(\sum_{i=1}^kr_i^2-\frac{k(k+1)^2}{4})$$符合自由度为k-1的卡方分布
2. 通常法：$$\frac{(N-1)\Gamma_{\mathfrak x^2}}{N(k-1)-\Gamma_{\mathfrak x^2}}$$其中$$\Gamma_{\mathfrak x^2}$$y由上面得到，该式子符合第一自由度为k-1第二自由度为(k-1)(N-1)的F分布

如果这个假设被拒绝，那么需要使用后续检验来进一步区分个算法，常用Nemenyi检验

Nemenyi检验首先计算平均序值差别的临界值阈$$CD=q_a\sqrt{\frac{k(k+1)}{6N}}$$，如果两个算法的平均序值超过这一临界值阈CD，则以相应的置信度拒绝**两个算法性能相同**的假设，该方式还可以用Friedman检验图得到。



## 参考

周志华 《机器学习》 清华大学出版社

